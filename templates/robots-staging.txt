#
# robots.txt - Staging Site
#
# This file blocks all search engine crawlers from indexing staging sites.
# Staging sites should NEVER be indexed by search engines to prevent:
#   - Duplicate content penalties
#   - Exposure of test/dummy data
#   - SEO confusion between staging and production URLs
#
# Generated by: NWP (stg2live.sh)
# Environment: Staging
# Last updated: 2026-01-09
#

# Block all search engines
User-agent: *
Disallow: /

# Block specific AI crawlers
User-agent: GPTBot
Disallow: /

User-agent: ClaudeBot
Disallow: /

User-agent: Google-Extended
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: anthropic-ai
Disallow: /

User-agent: Amazonbot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: cohere-ai
Disallow: /

User-agent: Omgilibot
Disallow: /

User-agent: FacebookBot
Disallow: /

# Block Internet Archive
User-agent: ia_archiver
Disallow: /

User-agent: archive.org_bot
Disallow: /

# Note: This robots.txt is ONE layer of defense.
# Staging sites also use:
#   - X-Robots-Tag: noindex, nofollow (HTTP header)
#   - Meta robots tags (page-level)
#   - Optional HTTP Basic Auth
