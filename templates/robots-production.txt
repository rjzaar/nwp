#
# robots.txt - Production Site
#
# This file controls search engine crawling behavior for production sites.
# It allows crawlers to index your content while protecting admin areas
# and system paths.
#
# Generated by: NWP (stg2live.sh)
# Environment: Production
# Last updated: 2026-01-09
#
# Documentation:
#   - Google robots.txt spec: https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt
#   - Drupal SEO best practices: https://www.drupal.org/docs/8/core/modules/system/robotstxt
#

# Default rules for all crawlers
User-agent: *

# Allow CSS, JS, and images for proper rendering in search results
Allow: /core/*.css$
Allow: /core/*.js$
Allow: /core/*.gif
Allow: /core/*.jpg
Allow: /core/*.jpeg
Allow: /core/*.png
Allow: /core/*.svg
Allow: /core/*.webp
Allow: /profiles/*.css$
Allow: /profiles/*.js$
Allow: /themes/*.css$
Allow: /themes/*.js$
Allow: /modules/*.css$
Allow: /modules/*.js$

# Block access to Drupal core directories
Disallow: /core/
Disallow: /profiles/
Disallow: /README.md
Disallow: /README.txt
Disallow: /web.config

# Block access to administrative paths
Disallow: /admin/
Disallow: /comment/reply/
Disallow: /filter/tips
Disallow: /node/add/
Disallow: /search/
Disallow: /user/register
Disallow: /user/password
Disallow: /user/login
Disallow: /user/logout
Disallow: /user/

# Block access to specific file types
Disallow: /media/oembed
Disallow: /*/media/oembed

# Block index.php in URL
Disallow: /index.php/

# Block Drupal configuration and install files
Disallow: /CHANGELOG.txt
Disallow: /cron.php
Disallow: /INSTALL.txt
Disallow: /INSTALL.mysql.txt
Disallow: /INSTALL.pgsql.txt
Disallow: /INSTALL.sqlite.txt
Disallow: /install.php
Disallow: /LICENSES/
Disallow: /LICENSE.txt
Disallow: /MAINTAINERS.txt
Disallow: /update.php
Disallow: /UPGRADE.txt
Disallow: /authorize.php
Disallow: /xmlrpc.php

# Block database dumps and backups
Disallow: /*.sql$
Disallow: /*.sql.gz$
Disallow: /*.tar$
Disallow: /*.tar.gz$
Disallow: /*.tgz$
Disallow: /*.zip$

# Crawl rate limiting (1 second between requests)
# Helps prevent server overload from aggressive crawlers
Crawl-delay: 1

# AI Crawler Controls
# Uncomment to block AI training crawlers from using your content
# See: https://platform.openai.com/docs/gptbot
#
# User-agent: GPTBot
# Disallow: /
#
# User-agent: ClaudeBot
# Disallow: /
#
# User-agent: Google-Extended
# Disallow: /
#
# User-agent: CCBot
# Disallow: /
#
# User-agent: anthropic-ai
# Disallow: /
#
# User-agent: Amazonbot
# Disallow: /
#
# User-agent: ChatGPT-User
# Disallow: /
#
# User-agent: cohere-ai
# Disallow: /
#
# User-agent: Omgilibot
# Disallow: /
#
# User-agent: FacebookBot
# Disallow: /

# Sitemap location
# Replace [DOMAIN] with your actual domain during deployment
Sitemap: https://[DOMAIN]/sitemap.xml

# Notes for site administrators:
# 1. If you're using Drupal's Simple XML Sitemap module, ensure it's configured
# 2. The Sitemap URL above should match your actual sitemap location
# 3. Consider enabling AI bot blocking (uncomment above) if you don't want
#    your content used for AI training
# 4. Test your robots.txt at: https://www.google.com/webmasters/tools/robots-testing-tool
